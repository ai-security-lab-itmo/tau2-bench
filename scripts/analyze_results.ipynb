{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmark Results Analysis\n",
        "\n",
        "This notebook loads and analyzes benchmark results from CSV files generated by `run_benchmark_suite.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the CSV file\n",
        "csv_path = Path(\"../data/simulations/benchmark_results.csv\")\n",
        "\n",
        "if not csv_path.exists():\n",
        "    print(f\"CSV file not found: {csv_path}\")\n",
        "    print(\"Please run: python scripts/run_benchmark_suite.py\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"âœ… Loaded {len(df)} rows from {csv_path}\")\n",
        "    print(f\"Columns: {len(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summary statistics\n",
        "print(\"=\" * 80)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTotal unique configurations: {len(df)}\")\n",
        "print(f\"Domains: {df['domain'].nunique()} ({', '.join(df['domain'].unique())})\")\n",
        "print(f\"Tasks: {df['task'].nunique()}\")\n",
        "print(f\"User models: {df['user_model'].nunique()} ({', '.join(df['user_model'].unique())})\")\n",
        "print(f\"Agent models: {df['agent_model'].nunique()} ({', '.join(df['agent_model'].unique())})\")\n",
        "\n",
        "if 'avg_reward' in df.columns:\n",
        "    print(f\"\\nOverall average reward: {df['avg_reward'].mean():.4f}\")\n",
        "if 'pass^1' in df.columns:\n",
        "    print(f\"Overall pass^1: {df['pass^1'].mean():.4f}\")\n",
        "if 'avg_agent_cost' in df.columns and df['avg_agent_cost'].notna().any():\n",
        "    print(f\"Overall average agent cost: ${df['avg_agent_cost'].mean():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the full table\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METRICS TABLE\")\n",
        "print(\"=\" * 80)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter by domain\n",
        "domain = \"mail_rag_phishing\"  # Change this to filter by domain\n",
        "domain_df = df[df['domain'] == domain]\n",
        "print(f\"Results for domain '{domain}': {len(domain_df)} rows\")\n",
        "domain_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by user_model_params to see temperature effects\n",
        "if 'user_model_params' in df.columns:\n",
        "    # Parse temperature from JSON string\n",
        "    def extract_temperature(params_str):\n",
        "        try:\n",
        "            params = json.loads(params_str)\n",
        "            return params.get('temperature', None)\n",
        "        except:\n",
        "            return None\n",
        "    \n",
        "    df['temperature'] = df['user_model_params'].apply(extract_temperature)\n",
        "    \n",
        "    # Group by temperature and compute averages\n",
        "    if 'temperature' in df.columns:\n",
        "        temp_summary = df.groupby('temperature').agg({\n",
        "            'avg_reward': 'mean',\n",
        "            'pass^1': 'mean',\n",
        "            'avg_agent_cost': 'mean',\n",
        "            'num_trials': 'sum'\n",
        "        }).round(4)\n",
        "        print(\"Summary by Temperature:\")\n",
        "        print(temp_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different agent models\n",
        "if 'agent_model' in df.columns:\n",
        "    model_comparison = df.groupby('agent_model').agg({\n",
        "        'avg_reward': 'mean',\n",
        "        'pass^1': 'mean',\n",
        "        'avg_agent_cost': 'mean',\n",
        "        'num_trials': 'sum'\n",
        "    }).round(4)\n",
        "    print(\"Model Comparison:\")\n",
        "    print(model_comparison)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task performance breakdown\n",
        "task_perf = df.groupby(['domain', 'task']).agg({\n",
        "    'avg_reward': 'mean',\n",
        "    'pass^1': 'mean',\n",
        "    'num_trials': 'sum'\n",
        "}).round(4).sort_values('avg_reward', ascending=False)\n",
        "\n",
        "print(\"Task Performance (sorted by avg_reward):\")\n",
        "task_perf\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
